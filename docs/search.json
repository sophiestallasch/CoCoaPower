[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CoCoaPower",
    "section": "",
    "text": "CENTER FOR STATISTICAL CONSULTING & COACHING ON THE DESIGN OF RANDOMIZED TRIALS IN EDUCATION\n \n  \n   \n  \n    \n     Email\n  \n  \n    \n     University of Potsdam\n  \n  \n    \n     GitHub\n  \n\n\n\n \n\n\nWithout any doubt, successful education is a fundamental pillar of a society’s prosperity. Across the globe, policymakers, practitioners, and researchers increasingly prioritize evidence-based education to improve learning and teaching (Organisation for Economic Co-operation and Development, 2007; Pellegrini & Vivanet, 2021; Slavin, 2002). This also applies to Germany (Kultusministerkonferenz, 2016).\nThe overarching goal of CoCoaPower is to advance these efforts by strengthening our national capacities to generate robust empirical evidence on the impacts of educational and psychological interventions, as well as innovative programs, products, and services, which can be derived from strong randomized trials (RTs).\n“Strong” means well-designed, well-implemented, and well-analyzed (Spybrook, 2013). CoCoaPower focuses on laying the groundwork for the first aspect of a strong RT, namely a good design. After all, valid and reliable causal inferences about the actual effects of interventions presuppose RTs that demonstrate sufficient statistical power and precision—or, put differently, adequate design sensitivity (Lipsey, 1990).\n\n\n \nCoCoaPower seeks to enhance the methodological quality of individually randomized trials (IRTs), multisite randomized trials (MSRTs), and cluster randomized trials (CRTs). Specifically, CoCoaPower pools and provides resources and expertise on design decisions in general and power analysis in particular.\n\nContact us!\n\nCoCoaPower is organized around two main strands:\n\n\n\n STATISTICAL CONSULTING SERVICE\nWe directly support researchers in planning rigorous RTs.\nWe provide…\n\nconcrete results obtained from power analysis tailored to researchers’ target study designs.\nadvice on general design decisions (e.g., regarding the choice of a specific design or the selection of covariates).\n\n\n\n\n\n STATISTICAL COACHING INFRASTRUCTURE\nWe train researchers to develop well-designed RTs.\nWe provide…\n\nspecialized workshops on power analysis for various RT designs.\nweb-based tutorials on power analysis that guide researchers through the steps necessary to determine the relevant quantities of a robust study design.\n\n\n\n\n\nContact us!\n\n\nBACKGROUND\nCoCoaPower builds on previous and ongoing work in the context of MULTI-DES, a project series funded by the German Research Foundation (Grant 392108331). MULTI-DES is dedicated to compiling an extensive database of single- and multilevel design parameters and effect size benchmarks for K-12 students’ cognitive and socio-emotional outcomes in the German education system. These design parameters and effect size benchmarks are key to planning sufficiently powered and precise RTs and interpreting their results.\n\n\nCoCoaPower is funded through a grant awarded to Larry V. Hedges by the Yidan Prize Foundation in 2018.\n\n\n \n\n \nSend us your message. We’ll get in touch with you.\n\n\n\n\nFull name \n\n\nEmail address \n\n\nMessage\n\n\n\n Send\n\n\n\n \n\n\n\n\n\n\nReferences\n\nKultusministerkonferenz. (2016). Gesamtstrategie der Kultusministerkonferenz zum Bildungsmonitoring [Overall strategy of the Standing Conference of the Ministers of Education and Cultural Affairs for educational monitoring]. Wolters Kluwer. https://www.kmk.org/fileadmin/Dateien/veroeffentlichungen_beschluesse/2015/2015_06_11-Gesamtstrategie-Bildungsmonitoring.pdf\n\n\nLipsey, M. W. (1990). Design sensitivity: Statistical power for experimental research. SAGE Publications.\n\n\nOrganisation for Economic Co-operation and Development. (2007). Evidence in education: Linking research and policy. OECD Publishing. https://doi.org/10.1787/9789264033672-en\n\n\nPellegrini, M., & Vivanet, G. (2021). Evidence-based policies in education: Initiatives and challenges in Europe. ECNU Review of Education, 4(1), 25–45. https://doi.org/10.1177/2096531120924670\n\n\nSlavin, R. E. (2002). Evidence-based education policies: Transforming educational practice and research. Educational Researcher, 31(7), 15–21. https://doi.org/10.3102/0013189X031007015\n\n\nSpybrook, J. (2013). Introduction to special issue on design parameters for cluster randomized trials in education. Evaluation Review, 37(6), 435–444. https://doi.org/10.1177/0193841X14527758"
  },
  {
    "objectID": "workshops/index.html",
    "href": "workshops/index.html",
    "title": "OUR WORKSHOPS",
    "section": "",
    "text": "2022\nJune 9-10\nRandomized Field Trials\nLarry V. Hedges, Martin Brunner, & Sophie E. Stallasch\nUniversity of Potsdam, Germany\nMaterials | Website\n\n\n2024\nMarch 21\nPoweranalyse für experimentelle Designs in R [Power Analysis for Experimental Designs in R]\nSophie E. Stallasch\nEarly career conference of the Society for Empirical Educational Research (GEBF) 11th Annual Conference, Potsdam, Germany\nMaterials | Website\n\n\n\n \n\n\n\nTba"
  },
  {
    "objectID": "workshops/index.html#previous-workshops",
    "href": "workshops/index.html#previous-workshops",
    "title": "OUR WORKSHOPS",
    "section": "",
    "text": "2022\nJune 9-10\nRandomized Field Trials\nLarry V. Hedges, Martin Brunner, & Sophie E. Stallasch\nUniversity of Potsdam, Germany\nMaterials | Website\n\n\n2024\nMarch 21\nPoweranalyse für experimentelle Designs in R [Power Analysis for Experimental Designs in R]\nSophie E. Stallasch\nEarly career conference of the Society for Empirical Educational Research (GEBF) 11th Annual Conference, Potsdam, Germany\nMaterials | Website"
  },
  {
    "objectID": "workshops/index.html#upcoming-workshops",
    "href": "workshops/index.html#upcoming-workshops",
    "title": "OUR WORKSHOPS",
    "section": "",
    "text": "Tba"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "OUR PUBLICATIONS",
    "section": "",
    "text": "Check out our new preprint on PsyArXiv!\nBrunner, M., Stallasch, S. E., Artelt, C., & Lüdtke, O. (2024). An individual participant data meta-analysis to support power analyses for randomized intervention studies in preschool: Cognitive and socio-emotional learning outcomes. PsyArXiv. https://doi.org/10.31234/osf.io/dkw42\n\n\n\nOUR PUBLICATIONS\n\n  \n    \n    \n    \n    \n    \n  \n  \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n  \n  \n    \n    Previous\n  \n  \n    \n    Next\n  \n\n \n \nBrunner, M., Stallasch, S. E., Artelt, C., & Lüdtke, O. (2024). An individual participant data meta-analysis to support power analyses for randomized intervention studies in preschool: Cognitive and socio-emotional learning outcomes. PsyArXiv. https://doi.org/10.31234/osf.io/dkw42\nStallasch, S. E., Lüdtke, O., Artelt, C., Hedges, L. V., & Brunner, M. (2024). Single- and multilevel perspectives on covariate selection in randomized intervention studies on student achievement. Educational Psychology Review, 36, 112. https://doi.org/10.1007/s10648-024-09898-7\nStallasch, S. E. (2024). Optimizing power analysis for randomized experiments: Design parameters for student achievement [Doctoral dissertation]. University of Potsdam. https://doi.org/10.25932/publishup-62939\nBrunner, M., Stallasch, S. E., & Lüdtke, O. (2023). Empirical benchmarks to interpret intervention effects on student achievement in elementary and secondary school: Meta-analytic results from Germany. Journal of Research on Educational Effectiveness, 17(1), 119–157. https://doi.org/10.1080/19345747.2023.2175753\nStallasch, S. E., Lüdtke, O., Artelt, C., & Brunner, M. (2021). Multilevel design parameters to plan cluster-randomized intervention studies on student achievement in elementary and secondary school. Journal of Research on Educational Effectiveness, 14(1), 172–206. https://doi.org/10.1080/19345747.2020.1823539\nBrunner, M., Keller, U., Wenger, M., Fischbach, A., & Lüdtke, O. (2018). Between-school variation in students’ achievement, motivation, affect, and learning strategies: Results from 81 countries for planning group-randomized trials in education. Journal of Research on Educational Effectiveness, 11(3), 452–478. https://doi.org/10.1080/19345747.2017.1375584"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "THE CoCoaPower PROJECT GROUP—WHO WE ARE",
    "section": "",
    "text": "THE CoCoaPower PROJECT GROUP—WHO WE ARE\n\n\n\n\n\n\n\n\n  \n    SOPHIE E. STALLASCH, PhD\n   \n  \n\n    \n      \n      Email\n    \n    \n      \n      Website\n    \n    \n      \n      ORCID\n    \n    \n      \n      GitHub\n    \n  \n\n\nSophie is a postdoc at the chair of Quantitative Methods in Educational Sciences at the University of Potsdam. Her dissertation in the context of MULTI-DES focuses on design parameters for student achievement to optimize power analysis for RTs. Apart from experimental design and power analysis, her research interests also cover cognitive and socio-emotional development, multilevel and latent variable modeling, meta-analysis, and R programming.\n\n\n\n\n  \n    MARTIN BRUNNER, PhD\n   \n  \n\n    \n      \n      Email\n    \n    \n      \n      Website\n    \n    \n      \n      ORCID\n    \n  \n\n\nMartin is a full professor and leads the chair of Quantitative Methods in Educational Sciences at the University of Potsdam. His main research interests are cognitive and socio-emotional skills, large-scale assessments, and quantitative methods including integrative data analysis/meta-analysis and multilevel modeling. He has distinguished expertise in various fields of educational research and statistics. As the head of MULTI-DES, he demonstrates long-standing experiences in research design, in particular with regard to design parameters, effect size benchmarks, and power analysis.\n\n\n\n \n\n\n\nCoCoaPower is funded by Larry V. Hedges through a grant awarded to him  by the Yidan Prize Foundation in 2018.\n\n\n  \n    LARRY V. HEDGES, PhD\n   \n  \n\n    \n      \n      Email\n    \n    \n      \n      Website\n    \n    \n      \n      ORCID\n    \n  \n\nLarry is the board of trustees professor of statistics and data science and education and social policy, professor of psychology, professor of medical social sciences, and co-director of the STEPP Center at Northwestern University. He is a world-renowned scholar in statistics and quantitative evaluation research, particularly with respect to research design and power analysis including design parameters, as well as research synthesis/meta-analysis.\n\n\n \n\n\n\n\n\n\n Imprint\n\n\n\n\n\nSophie E. Stallasch\nUniversity of Potsdam\nDepartment of Educational Sciences\nQuantitative Methods in Educational Sciences\nKarl-Liebknecht-Str. 24-25\n14476 Potsdam\nGERMANY\nMail: stallasch@uni-potsdam.de"
  },
  {
    "objectID": "dp/index.html",
    "href": "dp/index.html",
    "title": "OUR DESIGN PARAMETERS & EFFECT SIZE BENCHMARKS",
    "section": "",
    "text": "Without good estimates, power analysis is only guesswork.\n\n\n— David M. Murray (1998, p. 350)\n\n \nThe virtue of a power analysis stands or falls with the quality of its input—true to the motto, “garbage in, garbage out.” Reasonable assumptions on design parameters and effect sizes are therefore key to planning strong experimental studies. Design parameters reflect the (clustered) variance structure of the interventions’ target outcome. They include intraclass correlation coefficients (ICCs) ρ that quantify outcome differences between clusters and R2 values that quantify the proportions of explained variance by covariates (at various levels).\nBasically, the estimates entered into power computations should match the features of the planned RT: its target population, the outcome of interest, the hierarchical structure and randomization scheme, the ensuing analysis, and any applied covariates (e.g., Bloom et al., 2007; Campbell et al., 2000; Cohen, 1988; Hedges & Hedberg, 2007; Lipsey et al., 2012; Murray, 1998; Schochet, 2008; Spybrook et al., 2016; Zhang et al., 2023). However, since the true quantities are unknown a priori by definition, the assumed values for design parameters and effect sizes are inevitably subject to uncertainty, no matter how informed and (empirically and theoretically) justified a researcher’s assumptions are (this is also referred to as the local optimization problem, Du & Wang (2016); Moerbeek & Teerenstra (2016)].\n \n\n\nTo optimally support evaluation researchers in designing strong RTs that allow valid causal inferences about educational and psychological interventions, we have compiled extensive databases and resources of versatile single- and multilevel design parameters and effect size benchmarks (and corresponding standard errors), along with concise application and selection guidance.\n\n\n \n\n\n\n\n\n\n\n \n\n\n\n\n\n\nNote\n\n\n\nThere are several resources for empirical estimates of ρ and R2 for educational and psychological outcomes. An overview of existing international and German research on design parameters for student achievement can be found in Stallasch (2024), and – in more detail/meta-analytically aggregated – in Stallasch et al. (2021)/Stallasch et al. (2024). The research review in Brunner et al. (2024) also addresses socio-emotional learning outcomes. Similarly, Brunner et al. (2024) summarize available effect size benchmarks for student achievement.\n\n\n\n\n\n\nOur comprehensive set of multilevel design parameters for student achievement in elementary and secondary school\nIn Stallasch et al. (2021), we generated ρ and R2 values with corresponding standard errors for student achievement as the intervention’s target outcome. We specified two-level (students within schools) and three-level (students within classrooms within schools) latent covariate models to analyze design parameters for a broad array of mathematical-scientific, verbal, and domain-general achievement outcomes. The estimates apply to several (sub)populations across the entire school career from Grade 1 to 12. We cover the three most important covariate sets: pretests, sociodemographic characteristics, and the combination thereof. We utilized data from five national probability samples of three longitudinal German large-scale assessments (DESI, NEPS, PISA-I+ 2003), and additionally summarized the estimates in terms of normative distributions.\nThe design parameters are presented in an interactive Excel file. This Supplemental Online Material B comes with a detailed introduction on the application scopes for the various sets of estimates. The document can be downloaded from the study’s OSF repo or the Journal’s website.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Previews of the design parameters from Stallasch et al. (2021)\n\n\n\n\n\n  \n  Full collection\n\n\n\n\n\nOur (meta-analytic) single- and multilevel design parameters and selection guidelines for various covariate types, combinations, and time lags\nIn Stallasch et al. (2024), we specifically focused on covariates in randomized trials on student achievement. We accumulated a vast collection of R2 values with corresponding standard errors. Specifically, we referred to influential psychometric heuristics to analyze design parameters and develop selection guidelines for covariate types of varying bandwidth-fidelity, covariate combinations to quantify their incremental validities, and covariate time lags of 1 to 7 years to examine their validity degradation. We applied (manifest) single-level (students assumed to be independent), two-level (students within schools), and three-level (students within classrooms within schools) modeling and considered various mathematical-scientific and verbal achievement outcomes. Data stemmed from six national probability samples of three longitudinal German large-scale assessments (DESI, NEPS, PISA-I+ 2003, PISA-I+ 2012). The empirically estimated design parameters were meta-analytically integrated and employed in precision simulations.\nThe design parameters (and simulation results) are listed in interactive Excel files. The empirical estimates can be found in Online Supplemental Material E, and the meta-analytic estimates in Online Supplemental Material F (and the simulation results in Online Supplemental Material G). These resources can be downloaded from the study’s OSF repo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Previews of the empirically estimated design parameters from Stallasch et al. (2024)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Previews of the meta-analytically integrated design parameters from Stallasch et al. (2024)\n\n\n\n\n\n  \n  Full collection\n\n\n\n\n\nOur brand-new preprint with (meta-analytic) design parameters for cognitive and socio-emotional learning outcomes in preschool\nIn Brunner et al. (2024), we estimated and meta-analyzed ρ and R2 values for preschool children’s cognitive as well as socio-emotional learning outcomes. For this purpose, we drew on a systematic collection of individual participant data from 4 German probability samples of 2- to 6-year-olds. The estimates are relevant for planning single-level (e.g., in non-clustered lab-based settings), two-level (children nested within daycare centers), and three-level (children nested within groups within daycare centers) RTs. The analyzed outcomes are assessed with three methods (standardized tests, parent ratings, and educator ratings). We also considered vital covariate sets (baseline measures, sociodemographic characteristics, and their combination).\nBoth the empirically estimated and meta-analytically integrated design parameters are compiled in an Excel file featuring interactive tables with various filtering and selection functionalities. This allows users to easily access the design parameters that align with specific characteristics of the target intervention. The respective Online Supplemental Material B can be downloaded from the study’s OSF repo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Previews of the design parameters from Brunner et al. (2024)\n\n\n\n\n\n  \n  Full collection\n\n\n\n\n\n\n\n\nOur novel (meta-analytic) effect sizes to interpret results from intervention studies on student achievement across the entire school career\nIn Brunner et al. (2023), we gathered a vast collection of meta-analytically aggregated effect size benchmarks for various achievement outcomes of 1st to 12th graders–a resource that is especially valuable as it is the first of its kind in the German school system. Specifically, we generated estimates of normative expectations for students’ academic growth as well as of performance gaps between weak and average schools or between policy-relevant, demographically defined groups. We used longitudinal data from six large probability samples of four German large-scale assessments (NEPS, PISA-I+ 2003, PISA-I+ 2012, and DESI).\nThe effect size benchmarks are listed in Tables 1 to 5 of the article, available on the Journal’s website.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Previews of the effect size benchmarks from Brunner et al. (2023)\n\n\n\n\n\n  \n  Full collection\n\n\nFurther effect size benchmarks (e.g., from previous research, for specific school types and federal states) are documented in various Excel files. The respective Online Supplemental Materials B, C, and D can be downloaded from the study’s OSF repo or from the Journal’s website.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Previews of further effect size benchmarks from Brunner et al. (2023)\n\n\n\n\n\n  \n  Full collection\n\n\n\n\n\n\n\n\nOur world-wide diversified multilevel design parameters for student achievement, affect and motivation, and learning strategies\nIn their cross-country research, Brunner et al. (2024) estimated a broad spectrum of two-level (students within schools) ρ and R2 values for 15-year-olds’ domain-specific and domain-general academic outcomes. The authors capitalized on representative, cross-sectional large-scale assessment data for 81 nations and economies from five PISA cycles in the years 2000, 2003, 2006, 2009, and 2012. Sociodemographic characteristics were employed as covariates. The estimates are also summarized in terms of normative distributions.\nThe design parameters are provided as an extensive Excel file, which can be downloaded from the Journal’s website.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Previews of the design parameters from Brunner et al. (2018)\n\n\n\n\n\n  \n  Full collection"
  },
  {
    "objectID": "dp/index.html#design-parameters-for-the-german-school-context",
    "href": "dp/index.html#design-parameters-for-the-german-school-context",
    "title": "OUR DESIGN PARAMETERS & EFFECT SIZE BENCHMARKS",
    "section": "",
    "text": "Our comprehensive set of multilevel design parameters for student achievement in elementary and secondary school\nIn Stallasch et al. (2021), we generated ρ and R2 values with corresponding standard errors for student achievement as the intervention’s target outcome. We specified two-level (students within schools) and three-level (students within classrooms within schools) latent covariate models to analyze design parameters for a broad array of mathematical-scientific, verbal, and domain-general achievement outcomes. The estimates apply to several (sub)populations across the entire school career from Grade 1 to 12. We cover the three most important covariate sets: pretests, sociodemographic characteristics, and the combination thereof. We utilized data from five national probability samples of three longitudinal German large-scale assessments (DESI, NEPS, PISA-I+ 2003), and additionally summarized the estimates in terms of normative distributions.\nThe design parameters are presented in an interactive Excel file. This Supplemental Online Material B comes with a detailed introduction on the application scopes for the various sets of estimates. The document can be downloaded from the study’s OSF repo or the Journal’s website.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Previews of the design parameters from Stallasch et al. (2021)\n\n\n\n\n\n  \n  Full collection\n\n\n\n\n\nOur (meta-analytic) single- and multilevel design parameters and selection guidelines for various covariate types, combinations, and time lags\nIn Stallasch et al. (2024), we specifically focused on covariates in randomized trials on student achievement. We accumulated a vast collection of R2 values with corresponding standard errors. Specifically, we referred to influential psychometric heuristics to analyze design parameters and develop selection guidelines for covariate types of varying bandwidth-fidelity, covariate combinations to quantify their incremental validities, and covariate time lags of 1 to 7 years to examine their validity degradation. We applied (manifest) single-level (students assumed to be independent), two-level (students within schools), and three-level (students within classrooms within schools) modeling and considered various mathematical-scientific and verbal achievement outcomes. Data stemmed from six national probability samples of three longitudinal German large-scale assessments (DESI, NEPS, PISA-I+ 2003, PISA-I+ 2012). The empirically estimated design parameters were meta-analytically integrated and employed in precision simulations.\nThe design parameters (and simulation results) are listed in interactive Excel files. The empirical estimates can be found in Online Supplemental Material E, and the meta-analytic estimates in Online Supplemental Material F (and the simulation results in Online Supplemental Material G). These resources can be downloaded from the study’s OSF repo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Previews of the empirically estimated design parameters from Stallasch et al. (2024)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Previews of the meta-analytically integrated design parameters from Stallasch et al. (2024)\n\n\n\n\n\n  \n  Full collection\n\n\n\n\n\nOur brand-new preprint with (meta-analytic) design parameters for cognitive and socio-emotional learning outcomes in preschool\nIn Brunner et al. (2024), we estimated and meta-analyzed ρ and R2 values for preschool children’s cognitive as well as socio-emotional learning outcomes. For this purpose, we drew on a systematic collection of individual participant data from 4 German probability samples of 2- to 6-year-olds. The estimates are relevant for planning single-level (e.g., in non-clustered lab-based settings), two-level (children nested within daycare centers), and three-level (children nested within groups within daycare centers) RTs. The analyzed outcomes are assessed with three methods (standardized tests, parent ratings, and educator ratings). We also considered vital covariate sets (baseline measures, sociodemographic characteristics, and their combination).\nBoth the empirically estimated and meta-analytically integrated design parameters are compiled in an Excel file featuring interactive tables with various filtering and selection functionalities. This allows users to easily access the design parameters that align with specific characteristics of the target intervention. The respective Online Supplemental Material B can be downloaded from the study’s OSF repo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Previews of the design parameters from Brunner et al. (2024)\n\n\n\n\n\n  \n  Full collection"
  },
  {
    "objectID": "dp/index.html#effect-size-benchmarks-for-the-german-school-context",
    "href": "dp/index.html#effect-size-benchmarks-for-the-german-school-context",
    "title": "OUR DESIGN PARAMETERS & EFFECT SIZE BENCHMARKS",
    "section": "",
    "text": "Our novel (meta-analytic) effect sizes to interpret results from intervention studies on student achievement across the entire school career\nIn Brunner et al. (2023), we gathered a vast collection of meta-analytically aggregated effect size benchmarks for various achievement outcomes of 1st to 12th graders–a resource that is especially valuable as it is the first of its kind in the German school system. Specifically, we generated estimates of normative expectations for students’ academic growth as well as of performance gaps between weak and average schools or between policy-relevant, demographically defined groups. We used longitudinal data from six large probability samples of four German large-scale assessments (NEPS, PISA-I+ 2003, PISA-I+ 2012, and DESI).\nThe effect size benchmarks are listed in Tables 1 to 5 of the article, available on the Journal’s website.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Previews of the effect size benchmarks from Brunner et al. (2023)\n\n\n\n\n\n  \n  Full collection\n\n\nFurther effect size benchmarks (e.g., from previous research, for specific school types and federal states) are documented in various Excel files. The respective Online Supplemental Materials B, C, and D can be downloaded from the study’s OSF repo or from the Journal’s website.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Previews of further effect size benchmarks from Brunner et al. (2023)\n\n\n\n\n\n  \n  Full collection"
  },
  {
    "objectID": "dp/index.html#international-design-parameters",
    "href": "dp/index.html#international-design-parameters",
    "title": "OUR DESIGN PARAMETERS & EFFECT SIZE BENCHMARKS",
    "section": "",
    "text": "Our world-wide diversified multilevel design parameters for student achievement, affect and motivation, and learning strategies\nIn their cross-country research, Brunner et al. (2024) estimated a broad spectrum of two-level (students within schools) ρ and R2 values for 15-year-olds’ domain-specific and domain-general academic outcomes. The authors capitalized on representative, cross-sectional large-scale assessment data for 81 nations and economies from five PISA cycles in the years 2000, 2003, 2006, 2009, and 2012. Sociodemographic characteristics were employed as covariates. The estimates are also summarized in terms of normative distributions.\nThe design parameters are provided as an extensive Excel file, which can be downloaded from the Journal’s website.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Previews of the design parameters from Brunner et al. (2018)\n\n\n\n\n\n  \n  Full collection"
  }
]