@article{brunner.etal2025,
  title = {An Individual Participant Data Meta-Analysis to Support Power Analyses for Randomized Intervention Studies in Preschool: {{Cognitive}} and Socio-Emotional Learning Outcomes},
  shorttitle = {An {{Individual Participant Data Meta-Analysis}} to {{Support Power Analyses}} for {{Randomized Intervention Studies}} in {{Preschool}}},
  author = {Brunner, Martin and Stallasch, Sophie E. and Artelt, Cordula and L{\"u}dtke, Oliver},
  year = 2025,
  month = mar,
  journal = {Educational Psychology Review},
  volume = {37},
  number = {1},
  pages = {6},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {1040-726X, 1573-336X},
  doi = {10.1007/s10648-024-09981-z},
  url = {https://link.springer.com/10.1007/s10648-024-09981-z},
  urldate = {2025-07-10},
  abstract = {Abstract          There is a need for robust evidence about which educational interventions work in preschool to foster children's cognitive and socio-emotional learning (SEL) outcomes. Lab-based individually randomized experiments can develop and refine such interventions, and field-based randomized experiments (e.g., cluster randomized trials) evaluate their effectiveness in real-world daycare center settings. Applying reliable estimates of design parameters in the context of a priori power analyses is essential to ensure that the sample size of these studies is adequate to support strong statistical conclusions regarding the strength of the intervention effect. However, there is little knowledge on relevant design parameters with preschool children. We therefore utilized a systematic collection of individual participant data from four German probability samples (554\,{$\leq$}\,N\,{$\leq$}\,2928) with preschool children (aged two to six years) to estimate and meta-analyze design parameters. These parameters are relevant for planning single-level (e.g., in non-clustered lab-based settings), two-level (children nested in daycare centers), and three-level (children nested in groups, with groups nested in daycare centers) randomized intervention studies targeting cognitive and SEL outcomes assessed with three methods (standardized tests, parent ratings, and educator ratings). The design parameters depict between-group and -center differences as well as the proportion of variance in the outcomes explained by different covariate sets (socio-demographic characteristics, baseline measures, and their combination) at the child, group, and center level. In conclusion, this paper provides a rich source of design parameters, recommendations, and illustrations to support a priori power analyses for randomized intervention studies in early childhood education research.},
  copyright = {https://creativecommons.org/licenses/by/4.0},
  langid = {english},
  file = {C:\Users\Sophie_Stallasch\Zotero\storage\3ZCMYW34\Brunner et al. - 2025 - An Individual Participant Data Meta-Analysis to Support Power Analyses for Randomized Intervention S.pdf}
}

@article{bloom.etal2007,
  title = {Using Covariates to Improve Precision for Studies That Randomize Schools to Evaluate Educational Interventions},
  author = {Bloom, Howard S. and Richburg-Hayes, Lashawn and Black, Alison Rebeck},
  date = {2007-01-03},
  journaltitle = {Educational Evaluation and Policy Analysis},
  shortjournal = {EDUCATIONAL EVALUATION AND POLICY ANALYSIS},
  volume = {29},
  number = {1},
  pages = {30--59},
  issn = {0162-3737, 1935-1062},
  doi = {10.3102/0162373707299550},
  url = {http://epa.sagepub.com/content/29/1/30},
  urldate = {2016-08-30},
  abstract = {This article examines how controlling statistically for baseline covariates, especially pretests, improves the precision of studies that randomize schools to measure the impacts of educational interventions on student achievement. Empirical findings from five urban school districts indicate that (1) pretests can reduce the number of randomized schools needed for a given level of precision to about half of what would be needed otherwise for elementary schools, one fifth for middle schools, and one tenth for high schools, and (2) school-level pretests are as effective in this regard as student-level pretests. Furthermore, the precision-enhancing power of pretests (3) declines only slightly as the number of years between the pretest and posttests increases; (4) improves only slightly with pretests for more than 1 baseline year; and (5) is substantial, even when the pretest differs from the posttest. The article compares these findings with past research and presents an approach for quantifying their uncertainty.},
  langid = {english},
  keywords = {cluster randomization,educational interventions,Intraclass correlation,precision,pretests,randomizing schools},
  file = {C:\Users\Sophie_Stallasch\Zotero\storage\UFBIPEZ4\30.html}
}

@article{brunner.etal2018,
  title = {Between-School Variation in Students' Achievement, Motivation, Affect, and Learning Strategies: {{Results}} from 81 Countries for Planning Group-Randomized Trials in Education},
  shorttitle = {Between-{{School Variation}} in {{Students}}' {{Achievement}}, {{Motivation}}, {{Affect}}, and {{Learning Strategies}}},
  author = {Brunner, Martin and Keller, Ulrich and Wenger, Marina and Fischbach, Antoine and Lüdtke, Oliver},
  date = {2018},
  journaltitle = {Journal of Research on Educational Effectiveness},
  shortjournal = {Journal of Research on Educational Effectiveness},
  volume = {11},
  number = {3},
  pages = {452--478},
  issn = {1934-5747, 1934-5739},
  doi = {10.1080/19345747.2017.1375584},
  url = {https://www.tandfonline.com/doi/full/10.1080/19345747.2017.1375584},
  urldate = {2021-09-30},
  langid = {english},
  file = {C:\Users\Sophie_Stallasch\Zotero\storage\T8P3ZBHA\Brunner et al. - 2018 - Between-School Variation in Students' Achievement,.pdf}
}

@article{brunner.etal2023a,
  title = {Empirical Benchmarks to Interpret Intervention Effects on Student Achievement in Elementary and Secondary School: {{Meta-analytic}} Results from {{Germany}}},
  shorttitle = {Empirical {{Benchmarks}} to {{Interpret Intervention Effects}} on {{Student Achievement}} in {{Elementary}} and {{Secondary School}}},
  author = {Brunner, Martin and Stallasch, Sophie E. and Lüdtke, Oliver},
  date = {2023},
  journaltitle = {Journal of Research on Educational Effectiveness},
  shortjournal = {Journal of Research on Educational Effectiveness},
  volume = {17},
  number = {1},
  pages = {119--157},
  issn = {1934-5747, 1934-5739},
  doi = {10.1080/19345747.2023.2175753},
  url = {https://www.tandfonline.com/doi/full/10.1080/19345747.2023.2175753},
  urldate = {2024-05-16},
  langid = {english},
  file = {C:\Users\Sophie_Stallasch\Zotero\storage\YRRG7JHZ\Brunner et al. - 2024 - Empirical Benchmarks to Interpret Intervention Eff.pdf}
}

@article{campbell.etal2000,
  title = {Sample Size Calculations for Cluster Randomised Trials},
  author = {Campbell, Marion and Grimshaw, Jeremy and Steen, Nick and {Changing Professional Practice in Europe Group (EU BIOMED II Concerted Action)}},
  date = {2000-01},
  journaltitle = {Journal of Health Services Research \& Policy},
  shortjournal = {J Health Serv Res Policy},
  volume = {5},
  number = {1},
  pages = {12--16},
  issn = {1355-8196, 1758-1060},
  doi = {10.1177/135581960000500105},
  url = {http://journals.sagepub.com/doi/10.1177/135581960000500105},
  urldate = {2023-10-17},
  abstract = {Objectives:               Cluster randomised trials, in which groups of individuals are randomised, are increasingly being used in the health field. Adopting a clustered approach has implications for the design of such trials, and sample size calculations need to be inflated to accommodate for the clustering effect. Reliable estimates of intracluster correlation coefficients (ICCs) are required for robust sample size calculations to be made; however, little empirical evidence is available on their likely size, and on factors which influence their magnitude. The aim of this study was to generate empirical estimates of ICCs and to explore factors which may affect their magnitude.                                         Methods:               Empirical estimates of ICCs were calculated for both process variables and patient outcomes from a number of datasets of primary and secondary care implementation studies.                                         Results:               Estimates of ICCs varied according to setting and type of outcome. Estimates of ICCs for process variables were higher than those for patient outcomes, and estimates derived from secondary care were higher than those from primary care. ICCs for process variables in primary care were of the order of 0.05–0.15, whilst those in secondary care were of the order of 0.3. Estimates for patient outcomes in primary care were generally lower than 0.05.                                         Conclusions:               Adopting cluster randomisation has implications for the design, size and analysis of clinical trials. This study gives an insight into the potential size of ICCs in primary and secondary care, and provides a practical guide to researchers to aid the planning of future studies in this area.},
  langid = {english}
}

@book{cohen1988,
  title = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  date = {1988},
  edition = {2},
  publisher = {L. Erlbaum Associates},
  location = {Hillsdale, N.J},
  isbn = {978-0-8058-0283-2},
  langid = {english},
  pagetotal = {567},
  keywords = {Probabilities,Social sciences,Statistical methods,Statistical power analysis}
}

@article{du.wang2016,
  title = {A {{Bayesian}} Power Analysis Procedure Considering Uncertainty in Effect Size Estimates from a Meta-Analysis},
  author = {Du, Han and Wang, Lijuan},
  date = {2016-09-02},
  journaltitle = {Multivariate Behavioral Research},
  shortjournal = {Multivariate Behavioral Research},
  volume = {51},
  number = {5},
  pages = {589--605},
  issn = {0027-3171, 1532-7906},
  doi = {10.1080/00273171.2016.1191324},
  url = {https://www.tandfonline.com/doi/full/10.1080/00273171.2016.1191324},
  urldate = {2023-09-15},
  langid = {english}
}

@article{hedges.hedberg2007,
  title = {Intraclass Correlation Values for Planning Group-Randomized Trials in Education},
  author = {Hedges, Larry V. and Hedberg, Eric Christopher},
  date = {2007-01-03},
  journaltitle = {Educational Evaluation and Policy Analysis},
  shortjournal = {EDUCATIONAL EVALUATION AND POLICY ANALYSIS},
  volume = {29},
  number = {1},
  pages = {60--87},
  issn = {0162-3737, 1935-1062},
  doi = {10.3102/0162373707299706},
  url = {http://epa.sagepub.com/content/29/1/60},
  urldate = {2016-08-30},
  abstract = {Experiments that assign intact groups to treatment conditions are increasingly common in social research. In educational research, the groups assigned are often schools. The design of group-randomized experiments requires knowledge of the intraclass correlation structure to compute statistical power and sample sizes required to achieve adequate power. This article provides a compilation of intraclass correlation values of academic achievement and related covariate effects that could be used for planning group-randomized experiments in education. It also provides variance component information that is useful in planning experiments involving covariates. The use of these values to compute the statistical power of group-randomized experiments is illustrated.},
  langid = {english},
  keywords = {cluster randomized trials,experiments,Intraclass correlation,SE/CI for ICC and R2,statistical power},
  file = {C:\Users\Sophie_Stallasch\Zotero\storage\SBUKXDXK\60.html}
}

@book{lipsey.etal2012,
  title = {Translating the Statistical Representation of the Effects of Education Interventions into More Readily Interpretable Forms},
  author = {Lipsey, Mark W. and Puzio, Kelly and Yun, Cathy and Hebert, Michael A. and Steinka-Fry, Kasia and Cole, Mikel W. and Roberts, Megan and Anthony, Karen S. and Busick, Matthew D.},
  date = {2012-11},
  publisher = {National Center for Special Education Research},
  url = {http://eric.ed.gov/?id=ED537446},
  urldate = {2016-08-29},
  abstract = {This paper is directed to researchers who conduct and report education intervention studies. Its purpose is to stimulate and guide them to go a step beyond reporting the statistics that emerge from their analysis of the differences between experimental groups on the respective outcome variables. With what is often very minimal additional effort, those statistical representations can be translated into forms that allow their magnitude and practical significance to be more readily understood by the practitioners, policymakers, and even other researchers who are interested in the intervention that was evaluated. (Contains 12 tables and 6 figures.)},
  langid = {english},
  keywords = {ACADEMIC achievement,Benchmarking,Control Groups,Cost Effectiveness,Experimental Groups,Intervention,Mathematics Achievement,National Competency Tests,Program Costs,Reading Achievement,Researchers,SPECIAL education,Statistical Significance},
  file = {C:\Users\Sophie_Stallasch\Zotero\storage\48PXLBMH\eric.ed.gov.html}
}

@book{lipsey1990,
  title = {Design Sensitivity: {{Statistical}} Power for Experimental Research},
  shorttitle = {Design Sensitivity},
  author = {Lipsey, Mark W.},
  date = {1990},
  publisher = {SAGE Publications},
  location = {Newbury Park, Calif},
  isbn = {978-0-8039-3062-9 978-0-8039-3063-6},
  pagetotal = {207},
  keywords = {Social sciences,Statistical methods}
}

@book{moerbeek.teerenstra2016,
  title = {Power Analysis of Trials with Multilevel Data},
  author = {Moerbeek, Mirjam and Teerenstra, Steven},
  date = {2016},
  series = {Chapman \& {{Hall}}/{{CRC}} Interdisciplinary Statistics Series},
  publisher = {CRC Press},
  location = {Boca Raton},
  isbn = {978-1-4987-2989-5},
  pagetotal = {268},
  keywords = {Mathematical statistics,Methodology,Statistics}
}

@book{murray1998,
  title = {Design and Analysis of Group-Randomized Trials},
  author = {Murray, David M.},
  date = {1998},
  series = {Monographs in Epidemiology and Biostatistics},
  number = {v. 27 [i.e. 29]},
  publisher = {Oxford University Press},
  location = {New York},
  isbn = {978-0-19-512036-3},
  pagetotal = {467},
  keywords = {Group-randomized trials}
}

@book{organisationforeconomicco-operationanddevelopment2007,
  title = {Evidence in Education: {{Linking}} Research and Policy},
  shorttitle = {Knowledge {{Management Evidence}} in {{Education}}},
  author = {{Organisation for Economic Co-operation and Development}},
  date = {2007},
  publisher = {OECD Publishing},
  location = {Paris},
  url = {https://doi.org/10.1787/9789264033672-en},
  urldate = {2019-06-29},
  abstract = {Education policies and systems in all OECD countries are coming under increasing pressure to show greater accountability and effectiveness and it is crucial that educational policy decisions are made based on the best evidence possible. This book brings together international experts on evidence-informed policy in education from a wide range of OECD countries. The report looks at the issues facing educational policy makers, researchers, and stakeholders teachers, media, parents in using evidence to best effect. It focuses on the challenge of effective brokering between policy makers and re.},
  isbn = {978-92-64-03367-2},
  langid = {english},
  annotation = {OCLC: 476187613}
}

@article{pellegrini.vivanet2021,
  title = {Evidence-Based Policies in Education: {{Initiatives}} and Challenges in {{Europe}}},
  shorttitle = {Evidence-{{Based Policies}} in {{Education}}},
  author = {Pellegrini, Marta and Vivanet, Giuliano},
  date = {2021-03},
  journaltitle = {ECNU Review of Education},
  shortjournal = {ECNU Review of Education},
  volume = {4},
  number = {1},
  pages = {25--45},
  issn = {2096-5311, 2632-1742},
  doi = {10.1177/2096531120924670},
  url = {http://journals.sagepub.com/doi/10.1177/2096531120924670},
  urldate = {2023-09-25},
  abstract = {Purpose:               This article examines the state of progress of evidence-based educational policies in Europe and identifies organizations for the generation and dissemination of evidence. Further, it discusses some of the most relevant challenges facing the development of evidence-informed education policies in Europe.                                         Design/Approach/Methods:               This article analyzes official documents by the European Commission (EC) and other organizations. Literature in the field of evidence-based education worldwide is examined to identify the primary challenges and issues related to the development of a culture of evidence in Europe’s education sector.                                         Findings:               The EC has recently prioritized evidence-informed policy and practice in education, increasingly encouraging member states to utilize evidence in the policy decision-making process. According to official documents, this process began in 2006 and has since enjoyed remarkable progress through several initiatives intended to spread a culture of evidence in education. However, several challenges and issues remain regarding the promotion of evidence-informed policymaking.                                         Originality/Value:               Having prioritized evidence-informed policy and practice, the EC strongly encourages the adoption of evidence in the policymaking process. This article provides a point of reference regarding the initiatives already undertaken and the challenges facing evidence-based educational policies and policymaking in Europe.},
  langid = {english},
  file = {C:\Users\Sophie_Stallasch\Zotero\storage\LTUN5R3X\Pellegrini und Vivanet - 2021 - Evidence-Based Policies in Education Initiatives .pdf}
}

@article{schochet2008,
  title = {Statistical Power for Random Assignment Evaluations of Education Programs},
  author = {Schochet, Peter Z.},
  date = {2008-01-03},
  journaltitle = {Journal of Educational and Behavioral Statistics},
  shortjournal = {JOURNAL OF EDUCATIONAL AND BEHAVIORAL STATISTICS},
  volume = {33},
  number = {1},
  pages = {62--87},
  issn = {1076-9986, 1935-1054},
  doi = {10.3102/1076998607302714},
  url = {http://jeb.sagepub.com/content/33/1/62},
  urldate = {2016-08-30},
  abstract = {This article examines theoretical and empirical issues related to the statistical power of impact estimates for experimental evaluations of education programs. The author considers designs where random assignment is conducted at the school, classroom, or student level, and employs a unified analytic framework using statistical methods from the literature. Focusing on standardized test scores of elementary school students, this article discusses appropriate precision standards and, for each design, the required number of schools to achieve those standards using empirical values of intraclass correlations, regression R2 values, and other parameters. Clustering effects vary by design but are typically large. Thus, large school samples are required for education trials, and many evaluations will only have sufficient power to detect precise impacts for relatively large subgroups of sites.},
  langid = {english},
  keywords = {evaluations of education programs,experimental designs,statistical power},
  file = {C:\Users\Sophie_Stallasch\Zotero\storage\SYW4SIMA\62.html}
}

@article{slavin2002,
  title = {Evidence-Based Education Policies: {{Transforming}} Educational Practice and Research},
  shorttitle = {Evidence-{{Based Education Policies}}},
  author = {Slavin, Robert E.},
  date = {2002-10},
  journaltitle = {Educational Researcher},
  volume = {31},
  number = {7},
  pages = {15--21},
  issn = {0013-189X, 1935-102X},
  doi = {10.3102/0013189X031007015},
  url = {http://journals.sagepub.com/doi/10.3102/0013189X031007015},
  urldate = {2018-09-09},
  abstract = {At the dawn of the 21st century, educational research is finally entering the 20th century. The use of randomized experiments that transformed medicine, agriculture, and technology in the 20th century is now beginning to affect educational policy. This article discusses the promise and pitfalls of randomized and rigorously matched experiments as a basis for policy and practice in education. It concludes that a focus on rigorous experiments evaluating replicable programs and practices is essential to build confidence in educational research among policymakers and educators. However, new funding is needed for such experiments and there is still a need for correlational, descriptive, and other disciplined inquiry in education. Our children deserve the best educational programs, based on the most rigorous evidence we can provide.},
  langid = {english}
}

@article{spybrook.etal2016c,
  title = {Progress in the Past Decade: {{An}} Examination of the Precision of Cluster Randomized Trials Funded by the {{U}}.{{S}}. {{Institute}} of {{Education Sciences}}},
  shorttitle = {Progress in the Past Decade},
  author = {Spybrook, Jessaca and Shi, Ran and Kelcey, Benjamin},
  date = {2016-07-02},
  journaltitle = {International Journal of Research \& Method in Education},
  volume = {39},
  number = {3},
  pages = {255--267},
  issn = {1743-727X, 1743-7288},
  doi = {10.1080/1743727X.2016.1150454},
  url = {http://www.tandfonline.com/doi/full/10.1080/1743727X.2016.1150454},
  urldate = {2019-05-22},
  abstract = {This article examines the statistical precision of cluster randomized trials (CRTs) funded by the Institute of Education Sciences (IES). Specifically, it compares the total number of clusters randomized and the minimum detectable effect size (MDES) of two sets of studies, those funded in the early years of IES (2002–2004) and those funded in the recent years (2011–2013). Overall, the average precision in terms of MDES of studies in the recent cohort was more than double that of the early cohort (i.e. 0.48 compared to 0.23). The findings suggest a consistent and substantial increase in the precision of CRTs funded by IES in the past decade which is a critical step towards designing studies that have the potential to yield high-quality evidence about the effectiveness of educational interventions.},
  langid = {english}
}

@article{spybrook2013,
  title = {Introduction to Special Issue on Design Parameters for Cluster Randomized Trials in Education},
  author = {Spybrook, Jessaca},
  date = {2013-12-01},
  journaltitle = {Evaluation Review},
  shortjournal = {Evaluation Review},
  volume = {37},
  number = {6},
  pages = {435--444},
  issn = {0193-841X},
  doi = {10.1177/0193841X14527758},
  url = {http://journals.sagepub.com/doi/abs/10.1177/0193841X14527758},
  urldate = {2017-02-03},
  langid = {english}
}

@article{stallasch.etal2021,
  title = {Multilevel Design Parameters to Plan Cluster-Randomized Intervention Studies on Student Achievement in Elementary and Secondary School},
  author = {Stallasch, Sophie E. and Lüdtke, Oliver and Artelt, Cordula and Brunner, Martin},
  date = {2021-01-02},
  journaltitle = {Journal of Research on Educational Effectiveness},
  shortjournal = {Journal of Research on Educational Effectiveness},
  volume = {14},
  number = {1},
  pages = {172--206},
  issn = {1934-5747, 1934-5739},
  doi = {10.1080/19345747.2020.1823539},
  url = {https://www.tandfonline.com/doi/full/10.1080/19345747.2020.1823539},
  urldate = {2021-06-16},
  langid = {english},
  file = {C:\Users\Sophie_Stallasch\Zotero\storage\A4S9DEFN\Stallasch et al. - 2021 - Multilevel Design Parameters to Plan Cluster-Rando.pdf}
}

@article{stallasch.etal2024,
  title = {Single- and Multilevel Perspectives on Covariate Selection in Randomized Intervention Studies on Student Achievement},
  author = {Stallasch, Sophie E. and Lüdtke, Oliver and Artelt, Cordula and Hedges, Larry V. and Brunner, Martin},
  date = {2024-09-25},
  journaltitle = {Educational Psychology Review},
  shortjournal = {Educ Psychol Rev},
  volume = {36},
  number = {4},
  pages = {112},
  issn = {1573-336X},
  doi = {10.1007/s10648-024-09898-7},
  url = {https://doi.org/10.1007/s10648-024-09898-7},
  urldate = {2024-10-17},
  abstract = {Well-chosen covariates boost the design sensitivity of individually and cluster-randomized trials. We provide guidance on covariate selection generating an extensive compilation of single- and multilevel design parameters on student achievement. Embedded in psychometric heuristics, we analyzed (a) covariate types of varying bandwidth-fidelity, namely domain-identical (IP), cross-domain (CP), and fluid intelligence (Gf) pretests, as well as sociodemographic characteristics (SC); (b) covariate combinations quantifying incremental validities of CP, Gf, and/or SC beyond IP; and (c) covariate time lags of 1–7~years, testing validity degradation in IP, CP, and Gf. Estimates from six German samples (1868\,≤\,N\,≤\,10,543) covering various outcome domains across grades 1–12 were meta-analyzed and included in precision simulations. Results varied widely by grade level, domain, and hierarchical level. In general, IP outperformed CP, which slightly outperformed Gf and SC. Benefits from coupling IP with CP, Gf, and/or SC were small. IP appeared most affected by temporal validity decay. Findings are applied in illustrative scenarios of study planning and enriched by comprehensive Online Supplemental Material (OSM) accessible via~the Open Science Framework (OSF; https://osf.io/nhx4w).},
  langid = {english},
  keywords = {Covariate selection,Design parameters,Individual participant data meta-analysis,Individually and cluster-randomized trials,Power analysis,Student achievement}
}

@misc{kultusministerkonferenz2016,
  title = {Gesamtstrategie Der {{Kultusministerkonferenz}} Zum {{Bildungsmonitoring}} [{{Overall}} Strategy of the {{Standing Conference}} of the {{Ministers}} of {{Education}} and {{Cultural Affairs}} for Educational Monitoring]},
  author = {Kultusministerkonferenz},
  date = {2016},
  url = {https://www.kmk.org/fileadmin/Dateien/veroeffentlichungen_beschluesse/2015/2015_06_11-Gesamtstrategie-Bildungsmonitoring.pdf},
  organization = {Wolters Kluwer}
}

@thesis{stallasch2024,
  type = {doctoral thesis},
  title = {Optimizing Power Analysis for Randomized Experiments: {{Design}} Parameters for Student Achievement},
  author = {Stallasch, Sophie E.},
  date = {2024},
  pages = {ix, 224},
  institution = {Universität Potsdam},
  doi = {10.25932/publishup-62939},
  abstract = {Randomized trials (RTs) are promising methodological tools to inform evidence-based reform to enhance schooling. Establishing a robust knowledge base on how to promote student achievement requires sensitive RT designs demonstrating sufficient statistical power and precision to draw conclusive and correct inferences on the effectiveness of educational programs and innovations. Proper power analysis is therefore an integral component of any informative RT on student achievement. This venture critically hinges on the availability of reasonable input variance design parameters (and their inherent uncertainties) that optimally reflect the realities around the prospective RT—precisely, its target population and outcome, possibly applied covariates, the concrete design as well as the planned analysis. However, existing compilations in this vein show far-reaching shortcomings. The overarching endeavor of the present doctoral thesis was to substantively expand available resources devoted to tweak the planning of RTs evaluating educational interventions. At the core of this thesis is a systematic analysis of design parameters for student achievement, generating reliable and versatile compendia and developing thorough guidance to support apt power analysis to design strong RTs. To this end, the thesis at hand bundles two complementary studies which capitalize on rich data of several national probability samples from major German longitudinal large-scale assessments. Study I applied two- and three-level latent (covariate) modeling to analyze design parameters for a wide spectrum of mathematical-scientific, verbal, and domain-general achievement outcomes. Three vital covariate sets were covered comprising (a) pretests, (b) sociodemographic characteristics, and (c) their combination. The accumulated estimates were additionally summarized in terms of normative distributions. Study II specified (manifest) single-, two-, and three-level models and referred to influential psychometric heuristics to analyze design parameters and develop concise selection guidelines for covariate (a) types of varying bandwidth-fidelity (domain-identical, cross-domain, fluid intelligence pretests; sociodemographic characteristics), (b) combinations quantifying incremental validities, and (c) time lags of 1- to 7-year-lagged pretests scrutinizing validity degradation. The estimates for various mathematical-scientific and verbal achievement outcomes were meta-analytically integrated and employed in precision simulations. In doing so, Studies I and II addressed essential gaps identified in previous repertoires in six major dimensions: Taken together, this thesis accumulated novel design parameters and deliberate guidance for RT power analysis (1) tailored to four German student (sub)populations across the entire school career from Grade 1 to 12, (2) matched to 21 achievement (sub)domains, (3) adjusted for 11 covariate sets enriched by empirically supported guidelines, (4) adapted to six RT designs, (5) suitable for latent and manifest analysis models, (6) which were cataloged along with quantifications of their associated uncertainties. These resources are complemented by a plethora of illustrative application examples to gently direct psychological and educational researchers through pivotal steps in the process of RT design. The striking heterogeneity of the design parameter estimates across all these dimensions constitutes the overall, joint key result of Studies I and II. Hence, this work convincingly reinforces calls for a close match between design parameters and the specific peculiarities of the target RT’s research context. All in all, the present doctoral thesis offers a—so far unique—nuanced and extensive toolkit to optimize power analysis for sound RTs on student achievement in the German (and similar) school context. It is of utmost importance that research does not tire to spawn robust evidence on what actually works to improve schooling. With this in mind, I hope that the emerging compendia and guidance contribute to the quality and rigor of our randomized experiments in psychology and education.}
}

@incollection{zhang.etal2023,
  title = {Foundational Methods: {{Power}} Analysis},
  shorttitle = {Foundational Methods},
  booktitle = {International Encyclopedia of Education},
  author = {Zhang, Qi and Spybrook, Jessaca and Kelcey, Benjamin and Dong, Nianbo},
  editor = {Tierney, Robert J and Rizvi, Fazal and Ercikan, Kadriye},
  date = {2023},
  edition = {4},
  pages = {784--791},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-12-818630-5.10088-0},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128186305100880},
  urldate = {2023-08-29},
  isbn = {978-0-12-818629-9},
  langid = {english}
}
